# Парсер для получения основной информации со страницы
## Описание алгоритма
### Входные данные:
URL страницы;
Список тегов, в которых, предположительно, содержится главная информация страницы.
### Выходные данные:
Файл, содержащий основную информацию страницы.
### Алгоритм
При исследовании веб-страниц, мной было замечено, что на большинстве сайтов основная информация на странице находится в 
тегах заголовков h1...h6 и тегах p. Поэтому для максимальной универсальности можно собирать весь текст внутри этих 
тегов, в результате на большинстве страниц будет собираться основная информация. 

Для парсинга используется встроенный в Python класс html.parser.HTMLParser. Он последовательно просматривает все 
теги страницы, открывающие и закрывающие. Для нашей задачи достаточно унаследоваться от этого парсера и перегрузить его
методы handle_starttag, handle_endtag и handle_text.

Класс MainInfoParser работает так:
* Если встретился открывающий тег и он принадлежит тегам основной информации, заносим его в стек тегов.
* Если в стеке есть хоть один тег, это значит, что мы находимся внутри тега основной информации, поэтому если встречает
ся текст, мы кладем его в результирующую строку с основной информацией.
* Если встречаем ссылку, то преобразуем ее атрибут href в ссылку в квадратных скобках и кладем его в результирующую строку
* Если встретился закрывающий тег и он принадлежит тегам основной информации, убираем его из стека тегов.

Затем результирующая строка передается в класс FileSaver, который форматирует текст по заданной ширине и записывает в 
директорию, соответствующую URL страницы.
## Запуск программы
Запустить программу можно с помощью команды
```bash
python ./src/main.py [URL страницы]
```
Настройки программы находятся в файле settings.json.
## Примеры работы
Примеры работы находятся в файле tests.rar
## Направление дальнейшего улучшения программы
1. Так как спарсить основную информацию с помощью программы получается не со всех сайтов, для них придется делать
отдельные парсеры
2. Можно добавить новые правила для форматирования результирующего файла
3. Можно добавить возможность парсить сразу несколько страниц, URL которых будет браться, например, из отдельного файла
4. Можно дать возможность пользователю выбрать путь для сохранения файлов
5. Можно добавить графический интерфейс